# Выполнил лабораторную работу студент группы 6233-010402D Гудков Сергей

## Apache NiFi

От Apache NiFi остались исключительно негативные ощущения. От проблем с получением файлов и выполнением фильтрации через SQL запрос, до постоянных вылетов (ноут перенагружался и nifi просто вставал) в момент исполнения пайплайна, хотя выделено было 4г оперативки (больше выделить не мог, докер тогда вообще не запускается). Даже сейчас, пока пишу этот текст - он снова встал и не хочет перезагружаться (открываться пользовательский интерфейс), хотя никакими задачами он сейчас не занят, нужно снова перезапускать контейнер, хотя запущен он один... 

### Пайплайн

1. **GetFile**
   - **Описание**: Читает файлы из указанной директории.
   - **Конфигурация**: Указана директория с файлами и паттерн для поиска файлов.

2. **UpdateAttribute**
   - **Описание**: Переименовывает имя файла, чтобы в результате образовался один итоговый файл вместо множества отдельных файлов.
   - **Конфигурация**: Добавлен новый шаблон для имени файла.

3. **SplitRecord**
   - **Описание**: Разделяет файл на строки, обеспечивая возможность дальнейшей построчной обработки.

4. **QueryRecord**
   - **Описание**: Выполняет фильтрацию строк, отбирая только те, в которых заполнены нужные атрибуты.
   - **SQL-запрос**: `SELECT * FROM FLOWFILE WHERE designation IS NOT NULL AND region_1 IS NOT NULL`

5. **UpdateRecord**
   - **Описание**: Заменяет все `null` значения в поле `price` на `0.0`.
   - **Конфигурация**: Добавлен параметр `/price` со значением `${field.value:replaceNull(0.0)}`.

6. **MergeRecord** (два этапа)
   - **Первый этап**: 
     - **Описание**: Объединяет строки в блоки по 1000–5000 записей, чтобы избежать переполнения очередей и ускорить обработку.
     - **Причина**: Большое количество строк приводило к переполнению очередей при попытке объединения всего файла за один этап.
   - **Второй этап**: 
     - **Описание**: Объединяет блоки, созданные на первом этапе, в один большой файл.

7. **PutFile**
   - **Описание**: Сохраняет итоговый файл на диск в заданной директории.

8. **PutElasticsearchHttpRecord**
   - **Описание**: Отправляет обработанные записи в Elasticsearch.
   - **Особенность**: Для избежания ошибок валидации вместо `PutElasticsearchHttp` использован `PutElasticsearchHttpRecord`, что позволило избежать конвертации формата.

  ## Airflow

В рамках данной работы был реализован пайплайн с использованием Apache Airflow для обработки CSV-файлов, фильтрации данных и их сохранения. Код был оптимизирован с учетом возможностей библиотеки pandas, что позволило упростить некоторые этапы. Итоговый DAG состоит из следующих задач (tasks), каждая из которых реализована с помощью Python-функции и декоратора `@task`, что позволило избежать необходимости использования `PythonOperator`.

### Описание задач

1. **read_and_concat_files** - считывание и объединение файлов:
   - Задача считывает все CSV-файлы из указанной директории, объединяет их в один DataFrame для дальнейшей обработки и сохраняет результат во временный файл.
   - Это позволяет упростить передачу данных между задачами, используя буферный CSV-файл.

2. **filter_non_null_rows** - фильтрация данных:
   - Удаляет строки, в которых в колонках `designation` или `region_1` содержатся `null` значения.
   - Результат сохраняется во временный файл для последующей обработки.

3. **replace_null_values** - замена значений `null`:
   - Заменяет все `null` значения в колонке `price` на `0.0`, чтобы избежать ошибок при дальнейшей обработке данных.
   - После выполнения этой задачи обновленные данные сохраняются в буферном файле.

4. **save_to_csv** - сохранение в итоговый файл:
   - Сохраняет результат обработки в итоговый CSV-файл, размещенный в выходной директории.
   - Этот этап позволяет подготовить данные для окончательного анализа или загрузки в другие системы.

5. **save_to_elasticsearch** - загрузка данных в Elasticsearch:
   - Загружает каждую строку обработанного DataFrame в Elasticsearch, преобразовывая строки в формат JSON.
   - Эта задача обеспечивает интеграцию с Elasticsearch для удобного поиска и анализа данных.

### Структура DAG

Все задачи объединены в единую последовательность выполнения в DAG. График зависимостей выглядит следующим образом:

```plaintext
read_and_concat_files >> filter_non_null_rows >> replace_null_values >> [save_to_csv, save_to_elasticsearch]
```
  
![image](https://github.com/user-attachments/assets/f4a433dc-0157-4be4-b90b-c6e92f94c669)



